


# La complexit√© algorithmique

## Introduction √† la th√©orie de la complexit√©

**P = NP** ?

En informatique th√©orique, la question ùëÉ=ùëÅùëÉ demande si tous les probl√®mes dont la solution peut √™tre v√©rifi√©e rapidement (classe NP) peuvent aussi √™tre r√©solus rapidement (classe P). Autrement dit, trouver la solution est-il aussi rapide que v√©rifier une solution d√©j√† donn√©e ?
Bien que cette question encore non r√©solue puisse sembler simple au premier coup d'oeil pour un esprit qui ne serait pas familier √† l'algorithmique, elle est en fait le premier des "probl√®mes du mill√©naire", l'un des sept probl√®mes math√©matiques dits insurmontables et pos√©s en 2000 par l'Institut de math√©matiques Clay aux Etats-Unis. Malgr√© un titre √† l'air de d√©fi impossible et une r√©compense d'un million de dollar fournie par l'Institut pour quiconque r√©solverait un de ces dits probl√®mes, le math√©maticien britannique Keith Devlin consid√®re le probl√®me **P = NP** ? comme le seul probl√®me compr√©hensible par tous. Loin d'avoir la pr√©tention de pouvoir faire gagner la r√©compense suite √† le lecture de cet article, ce dernier permettra de mieux aborder la question plus globale de la complexit√© des algorithmes afin de compendre notamment pourquoi **P = NP?** sera certainement encore un probl√®me du prochain mill√©naire.

Pour commencer, nous d√©finirons un algorithme comme *"un ensemble de r√®gles permettant de r√©soudre un probl√®me sur des donn√©es d'entr√©es. Cet ensemble de r√®gles d√©finit une s√©quence d'op√©rations qui se termine en un temps fini. Cependant, pour un probl√®me donn√© il peut exister plusieurs algorithmes possibles"* [^1]. La th√©orie de la complexit√© d√©coule alors d'un probl√®me tr√®s terre √† terre. En effet, dans les ann√©es 1960, les premiers gros ordinateurs se diffusent mais ces derniers, compar√©s aux machines dont nous disposons aujourd'hui, sont tr√®s lents et disposent de tr√®s peu de m√©moire vive. Tr√®s vite, les chercheurs se posent donc la question de la complexit√© des algorithmes: en combien de temps un algorithme peut-il s'ex√©cuter sur une machine donn√©e ? Surtout, un algorithme peut-il simplement tourner sur cette machine ? Finalement, quel algorithme choisir, lequel sera le plus optimis√© ?
Ces questions donnent donc lieu √† l'identification de deux types de complexit√©:

  - **complexit√© temporelle**: c'est la mesure du temps d'ex√©cution d'un algorithme en fonction de la taille de l'entr√©e
  - **complexit√© spatiale**: c'est la mesure de la m√©moire n√©cessaire √† l'ex√©cution d'un algorithme
 
En somme,  √©tudier la complexit√© d'un algorithme c'est calculer combien de temps il va prendre pour s'ex√©cuter en fonction d'une donn√©e de taille *n* en entr√©e. De plus, ici nous ne nous int√©resserons pas √† la complexit√© spatiale mais bien √† la complexit√© temporelle car bien que nos ordinateurs n'aient pas de m√©moire infinie, nous nous basrons sur le mod√®le de la machine de Turing (qui elle poss√®de une m√©moire infinie par d√©finition) que nous d√©taillerons ensuite. En pratique, la m√©moire d'un ordinateur, par sa taille, peut aujourd'hui √™tre consid√©r√©e comme infinie. De plus, nous pouvons la consid√©rer comme infinie de mani√®re th√©orique car elle est extensible par l'ajout de composants[^2]. 

Afin de mieux saisir ce que repr√©sente la complexit√© d'un algorithme dans la pratique, prenons en exemple le code suivant:
```python
def diviseurs(n:int):
    liste_diviseurs = []
    for i in range (1, n+1):
        if n%i == 0: #Soit si i est un diviseur de n
            liste_diviseurs.append(i)
    return liste_diviseurs
```
Ici, plusieurs op√©rations sont n√©cessaires pour ex√©cuter cet algorithme:

 - une affectation: liste_diviseurs=[ ]
 - n calculs de reste: n%i
 - n comparaisons: n%i == 0
 - x op√©rations d'ajouts √† la liste liste_diviseurs. x est donc au moins √©gal √† 1 (par exemple si n=1 puisque 1 ne peut avoir en diviseur que lui-m√™me) et au plus √† n (2 par exemple puisque 2 a comme diviseurs 1 et 2).

Le nombre d'op√©rations pour cet algorithme varie donc entre 2n+2 et 3n+1. Le nombre d'op√©rations n√©cessaires est proportionnel √† *n*. Si on affecte un temps constant √† une op√©ration √©l√©mentaire (ici l'affectation par exemple) alors le temps pour ex√©cuter l'algorithme est propotionnel √† n. 

Prenons en comparaison ce deuxi√®me algorithme:
```python
import math
def diviseurs(n:int):
    liste_diviseurs = []
    for i in range (1, int(math.sqrt(n))+1):
        if n%i == 0: #Soit si i est un diviseur de n
            liste_diviseurs.append(i)
            if n//i!=i: #Si le quotient de n par i est diff√©rent de i, alors on ajoute ce quotient 
            #comme nouveau diviseur de n
                liste_diviseurs.append(n//i)
    return liste_diviseurs
```
Ici, cet algorithme s'appuie sur le fait que si p est un diviseur de n alors on peut √©crire n=p√óq, avec p>=‚àön,  alors q est un diviseur de n inf√©rieur ou √©gal √† ‚àön. Pour les esprits moins scientifiques, pour r√©sumer le programme suivant, nous pouvons dire qu'il cherche tous les diviseurs inf√©rieurs ou √©gal √† ‚àön puisqu'une propri√©t√© math√©matique nous dit que n peut s'√©crire sous la forme p\*q (p et q entiers naturels) avec p forc√©ment inf√©rieur ou √©gal √† la racine de n. L'algorithme cherche donc d'abord √† trouver tous ces p. Ensuite, il ajoute tous les q. Pour cet algorithme, on fait donc les op√©rations suivantes:

- une affectation : liste_diviseurs= [ ]
- un calcul de racine carr√©e: math.sqrt(n)
- ‚àön calculs de reste: n%i
- ‚àön comparaisons: n%i==0
- Entre 1 et ‚àön calculs de quotient: n//i
- Entre 1 et ‚àön comparaisons: n//i !=i
- x op√©rations d'ajouts √† la liste liste_diviseurs. x est au moins √©gal √† 1 et au plus √† 2√ó‚àön.√ó

Le nombre d'op√©rations pour cet algorithme varie donc entre 2√ó‚àön+5 et 6√ó‚àön+2. Le nombre d'op√©rations est donc proportionnel √† ‚àön. 

On voit donc √† travers l'exemple de ces deux algorithmes que le nombre d'op√©rations peut varier fortement pour obtenir le m√™me r√©sultat en sortie, on a donc bien des diff√©rences en terme de complexit√©. Par exemple ici pour n=100 le nombre d'op√©rations avec le premier algorithme varie entre 202 et 301 et entre 25 et 62 pour le deuxi√®me. 

## Types de probl√®mes algorithmiques

Les probl√®mes algorithmiques se distinguent en deux types:

- Les probl√®mes dits d√©cisionnels de type bool√©ens, dont la r√©ponse est vraie ou faux, par exemple: "n a-t-il exactement deux diviseurs ?"
- Les probl√®mes dits de recherche d'une solution, par exemple: "Quels sont les diviseurs de n ?"

Toutefois, certains probl√®mes de recherche de solution peuvent √™tre transform√©s en probl√®mes d√©cisionnels dits √©quivalents. Si nous posons la question: "Existe-t-il une suite finie de nombre contenant tous les diviseurs de n?" le probl√®me est d'ordre d√©cisionnel mais il faudra bien le m√™me nombre d'√©tapes que pour le probl√®me de recherche √† la machine pour y r√©pondre. 

## Mod√®le de calcul utilis√©

Bien qu'il existe plusieurs m√©thodes de calcul, nous nous basons ici sur celui de la machine de Turing. Ce mod√®le de machine est un mod√®le th√©orique d√©fini par Alain Turing en 1936 afin de formaliser la notion de calculabilit√© et d'algorithme. Pour r√©sumer, la machine de Turing est compos√©e de trois √©l√©ments:

- un ruban infini (m√©moire de la machine)
- une t√™te de lecture et d'√©criture (qui lit et √©crit sur le ruban)
- un tableau de r√®gles autrement appel√© partie de contr√¥le

Le ruban est divis√© en plusieurs cases qui contiennent toutes un symbole, ici nous prendrons pour exemple les symboles 0, 1 ou un espace symbolis√© par "_". La t√™te de lecture et d'√©criture se d√©place de case en case et peut lire les symboles associ√©s ainsi qu'√©crire un nouveau symbole. Elle poss√®de √©galement un √©tat qui peut changer en fonction du tableau de r√®gles. C'est ce tableau qui indique √† la machine son √©tat actuel, quel symbole est lu, quel symbole √©crire ensuite, o√π se d√©placer et quel √©tat adopter. On peut mod√©liser cela par un exemple dans le tableau suivant: 

|Etat          | Symbole lu   |Ecrit         |Deplace       | Nouvel Etat  |
|:------------:|:------------:|:------------:|:------------:|:------------:|
|A |1|0|droite|A|
|A|_|_|Stop|HALT|

*Exemple de tableau des transitions*

Ici, si on soumet √† la machine le ruban 1 1 1 _ _ _ _, le ruban en sorti sera 0 0 0 _.

Pour la machine de Turing, on mesure la complexit√© temporelle par le nombre d'√©tapes √©l√©mentaires de calcul et la complexit√© spatiale par le nombre de cases du ruban qui sont utilis√©es durant le calcul (par analogie, on parlerait en unit√© de m√©moire pour un ordinateur). C'est donc cette mesure de complexit√© temporelle que nous utilisons dans l'exemple de la section pr√©c√©dente (code Python). Ici nous nous sommes int√©ress√©s √† une machine √† bande unique mais il est bien s√ªr possible d'en consid√©rer avec plusieurs bandes, ou m√™me avec une bande unique dite bifinie (innfinie √† droite et √† gauche). 

<img src="machine_de_turing.jpg" alt="alt text"> 

*Exemple de machine de Turing, source: Alan Turing et le d√©cryptage des codes secrets nazis, CNRS Le Journal*

## Machines d√©terministes, machines non d√©terministes

L'exemple pr√©c√©dent concernant la Machine de Turing √©tait celui d'une machine dite d√©terministe, c'est-√†-dire d'une machine o√π pour chaque √©tat et chaque entr√©e, il existe une seule transition possible. Dit plus simplement, cela signifie que si nous donnons la m√™me entr√©e √† la machine plusieurs fois, elle produira toujours le m√™me r√©sultat. 
Pour les machines non-d√©terministes, √† un √©tat donn√©, plusieurs transitions sont possibles pour la m√™me entr√©e. Afin de mieux comprendre on peut regarder les deux sch√©mas suivants: 
<img src="automate_deterministe.png" alt="alt text"> 

Ici un seul chemin est possible: si la machine lit a, elle passe √† q1, si elle lit b, elle passe √† q2...

<img src="automate_non_deterministe.png" alt="alt text"> 

Ici, en lisant a, la machine peut passer soit √† q1 soit √† q2, elle teste plusieurs chemins en m√™me temps. 

## Algorithmes efficaces, algorithme inefficaces

La premi√®re r√©f√©rence √† l'efficacit√© des algorithmes remonte √† 1956 lorsque le math√©maticien autrichien Kurt G√∂del √©crit √† son homologue hongrois von Neumann pour lui demander s'il existe un algorithme quadratique, c'est-√†-dire un algorithme dont le nombre d‚Äôop√©rations est proportionnel √† *n*¬≤ o√π *n* est la taille des donn√©es d'entr√©e, pour le probl√®me SAT (Satisfiabilit√© Bool√©enne) qui consiste √† d√©terminer si une formule bool√©enne peut √™tre √©valu√©e √† vrai en attribuant des valeurs de v√©rit√© (true/false) aux variables [^3]. 
Par la suite, c'est la d√©finition de Cobham [^4] ainsi que celle d'Edmonds [^5] qui s'imposent comme universelles: un algorithme efficace fonctionne en temps polynomial, soit si le nombre d'op√©rations qu'il effectue est born√© par une fonction de la forme :
*O(n<sup>k</sup>)* o√π : 
- *n* est la taille de l'entr√©e
- *k* est une constante

Ainsi, un algorithme est dit efficace si le nombre d'op√©ration effectu√© est de la forme de la fonction pr√©sent√©e ci-dessus, on dit alors qu'il est **dans P** (pour polynomial). Afin de mieux comprendre, nous pouvons consid√©rer le tableau suivant qui pr√©sente le temps n√©cessaire √† l'ex√©cution de plusieurs algorithmes en fonction des donn√©es en entr√©e :
|            | Nom courant  | Temps pour *n* = 10‚Å∂             | Remarques |
|:------------:|:--------------:|:-------------------------------------:|:-----------:|
|*O*(1)      |Temps constant| 1 ns                                | Le temps d'ex√©cution ne d√©pend pas des donn√©es trait√©es.|
|*O*(log *n*)|logarithmique | 10 ns                               |En pratique, cela correspond √† une ex√©cution quasi instantan√©e.|
|*O(n)*      |lin√©aire      | 1 ms                                |Le temps d'ex√©cution d'un tel algorithme ne devient sup√©rieur √† une minute que pour des donn√©es de taille comparable √† celle des m√©moires vives disponibles actuellement. Le probl√®me de la gestion de la m√©moire se posera donc avant celui de l'efficacit√© en temps.|
|*O*(*n*¬≤)   |quadratique   |15 mn                                |Cette complexit√© reste acceptable pour des donn√©es de taille moyenne (*n*<10‚Å∂) mais pas au-del√† avec la plupart des processeurs actuels.|
|*O*(*n<sup>k</sup>*)|polyn√¥miale   | 30 ans pour *k* = 3                 |Ici *n<sup>k</sup>* est le terme de plus haut degr√© d'un polyn√¥me en n.|
|*O*(*2<sup>n</sup>*)|exponentielle | plus de 10¬≥‚Å∞‚Å∞ ‚Å∞‚Å∞‚Å∞ milliards d'ann√©es|Un algorithme d'une telle complexit√© est impraticable sauf pour de tr√®s petites donn√©es (*n*<50). Algorithme inefficace.|

Table - *Ordre de grandeur des temps d'ex√©cution d'un probl√®me de taille 10‚Å∂ sur un ordinateur √† un milliard d'op√©rations par seconde*

On comprend donc gr√¢ce au tableau qu'un algorithme efficace ne peut √™tre que de la forme polyn√¥miale (au maximum avec n‚Å¥) pour pouvoir √™tre ex√©cutable. On pr√©cise toutefois que le temps n√©cessaire calcul√© ici ne prend pas en compte les performances techniques de la machine car on reste sur le m√™me ordre de grandeur. 

## Classes de complexit√©

Nous l'avons vu, un algorithme efficace se r√©sout en temps polynomial. Mais il existe d'autres ordres de grandeurs qu'on appelle alors classes de complexit√© en temps (il en existe aussi pour la m√©moire). Elles peuvent √™tre regroup√©es dans le sch√©ma suivant (toutes les classes de complexit√© n'y figurent pas, on choisit les prinncipales): 
<img src="classes_de_complexite.png" alt="alt text"> 

## Bibliographie : 


Sylvain Perifel, Complexit√© algorithmique, Ellipses, 2014

Olivier Carton, Langages formels, calculabilit√© et complexit√©, 2008 

 Alan Turing, On Computable Numbers, with an Application to the Entscheidungsproblem : Proceedings of the London Mathematical Society, London Mathematical Society, 1937
 
 Alan Turing et Jean-Yves Girard, La machine de Turing, Paris, √âditions du Seuil, 1995
 
## Sitographie

Complexity Zoo: https://complexityzoo.net/ 

[^1]: Courrier, N. (2021), *Complexit√© temporelle des algorithmes* \[Cours Magistral], Classe Pr√©paratoire aux Grandes Ecoles PTSI Lyc√©e Chaptal

[^2]: voir Sylvain Perifel, "La Machine de Turing", dans *Complexit√© algorithmique*, Ellipses, 2014, p.7

[^3]: voir la lettre de G√∂del √† Von Neumann en 1956 dans [¬´ G√∂del‚Äôs Lost Letter and P=NP : a personal view of the theory of computation¬ª](https://rjlipton.com/the-gdel-letter/), sur la page de Richard J. Lipton.

[^4]: Alan Cobham. ¬´ The Intrinsic Computational Difficulty of Functions ¬ª. In: *Logic, Methodology and Philosophy of Science, proceedings of the second International Congress, held in Jerusalem*, 1964.

[^5]: Jack Edmonds. ¬´ Paths, Trees, and Flowers ¬ª. In:  *Canad. J. Math* 17 (1965),p. 449‚Äì467
